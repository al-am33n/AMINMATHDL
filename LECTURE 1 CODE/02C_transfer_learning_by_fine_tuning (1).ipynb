{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcum4WIJJmM"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "In the context of deep learning, transfer learning is the process of using a pre-trained neural network as the starting point for a new task, rather than training a neural network from scratch. The idea is to use knowledge learned from one task to improve performance on a new, related task.\n",
        "\n",
        "The pre-trained neural network is first trained on a large dataset, and then the weights and biases of the network are used as a starting point for a new task. The pre-trained network is fine-tuned on a smaller dataset specific to the new task. This process can significantly reduce the amount of data and computational resources required to train a new deep learning model, while potentially also improving its performance.\n",
        "\n",
        "Transfer learning is widely used in computer vision and natural language processing tasks. It shines, and shows its value for On Convolutional Neural Networks and Transformers.\n",
        "\n",
        "This notebook shows how to transfer learning can run on a toy model. We use a dense (fully connected) feed-forward neural network with four layers. Generally speaking, such shallow and dense neural networks are not the best candidates for transfer learning.\n",
        "\n",
        "\n",
        "The main takeaway from this notebook is:\n",
        "1. Know how to run transfer learning in Keras\n",
        "\n",
        "\n",
        "This notebook is based on the [notebook](https://github.com/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb) associated with Geron chapter 11. It is presented here with modifications in accordance with the Apache 2.0 license."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFy7VWGuJJmN"
      },
      "source": [
        "## The Data - Fashion MNIST\n",
        "The data:\n",
        "https://keras.io/api/datasets/fashion_mnist/\n",
        "\n",
        "Fashion-MNIST is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n",
        "\n",
        "The classes are:\n",
        "\n",
        "Label |\tDescription\n",
        "--- | ---\n",
        "0|\tT-shirt/top\n",
        "1|\tTrouser\n",
        "2|\tPullover\n",
        "3|\tDress\n",
        "4|\tCoat\n",
        "5|\tSandal\n",
        "6|\tShirt\n",
        "7|\tSneaker\n",
        "8|\tBag\n",
        "9|\tAnkle boot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa4ndgf_JJmO"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtD1skDDJJmO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i99RCrBrJJmO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pM_s-y3JJmO"
      },
      "source": [
        "## Get the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJsbZ73CJJmP"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "# split the 60,000 training images into 55,000 training images and 5,000 validation images\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbL2HyivJJmP"
      },
      "source": [
        "## Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdnzDh5NJJmP",
        "outputId": "d4d4e3a4-30e6-4f96-f72e-8e4f39ae2727"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1bfaad6d450>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(X_train[0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS0W-5bNJJmP",
        "outputId": "a3a742a1-a64d-45e7-ef33-c5e9a568d480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50UjIF8GJJmP",
        "outputId": "caf46c30-76b8-4f31-8ca0-e00d7cfb4e67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ankle boot'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSYoIujJJmP",
        "outputId": "78b2fdcf-fd3f-4f07-99c6-4c5d76d0cb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.uint8'>\n"
          ]
        }
      ],
      "source": [
        "print(type(X_train[0,0,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPYaeA-NJJmP"
      },
      "source": [
        "## Preprocess the Data\n",
        "Notice that unlike the previous notebook, where MNIST was used, in this notebook we don't flatten the images to vectors at the preprocess stage.\n",
        "Instead, we flatten it as the first stage of the model.\n",
        "By the way, we could also add the preprocessing below to be part of the model. This is useful for stages that we expect to _always_ be required when running the model.\n",
        "\n",
        "Notice that the mean and standard deviation are calulated only over the training set, and then used for all datasets, including the validation and test sets. Can you say why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83BYmso2JJmQ"
      },
      "outputs": [],
      "source": [
        "# scale the values to 0 - 1 by using min-max normalization\n",
        "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxHau06PJJmQ"
      },
      "outputs": [],
      "source": [
        "# normalize the data such that each feature (each pixel) has a mean of zero\n",
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqoV6GrCJJmQ"
      },
      "source": [
        "## Split the Data to Demonstrate Transfer-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMjAxQymJJmQ"
      },
      "source": [
        "Let's split the fashion MNIST training set in two:\n",
        "* `X_train_A`: all images of all items except for **T-shirts/tops** and **pullovers** (classes **0** and **2**).\n",
        "* `X_train_B`: a much smaller training set of just the first 200 images of T-shirts/tops and pullovers.\n",
        "\n",
        "The validation set and the test set are also split this way, but without restricting the number of images.\n",
        "\n",
        "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (trousers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots) are somewhat similar to classes in set B (T-shirts/tops and pullovers). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ZlEx-dJJmQ"
      },
      "outputs": [],
      "source": [
        "pos_class_id = class_names.index(\"Pullover\")\n",
        "neg_class_id = class_names.index(\"T-shirt/top\")\n",
        "\n",
        "def split_dataset(X, y):\n",
        "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
        "    y_A = y[~y_for_B]\n",
        "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
        "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
        "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
        "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
        "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
        "\n",
        "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
        "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
        "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
        "X_train_B = X_train_B[:200]\n",
        "y_train_B = y_train_B[:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqGXLcPoJJmQ"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwk9yRnXJJmQ",
        "outputId": "c4d50604-0366-4ced-d977-6502bdbde33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1376/1376 [==============================] - 4s 3ms/step - loss: 1.1787 - accuracy: 0.6364 - val_loss: 0.6959 - val_accuracy: 0.7904\n",
            "Epoch 2/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.5907 - accuracy: 0.8157 - val_loss: 0.5118 - val_accuracy: 0.8348\n",
            "Epoch 3/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.4739 - accuracy: 0.8487 - val_loss: 0.4418 - val_accuracy: 0.8501\n",
            "Epoch 4/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.4172 - accuracy: 0.8636 - val_loss: 0.3986 - val_accuracy: 0.8644\n",
            "Epoch 5/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3819 - accuracy: 0.8735 - val_loss: 0.3723 - val_accuracy: 0.8699\n",
            "Epoch 6/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3575 - accuracy: 0.8790 - val_loss: 0.3519 - val_accuracy: 0.8769\n",
            "Epoch 7/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3402 - accuracy: 0.8841 - val_loss: 0.3383 - val_accuracy: 0.8774\n",
            "Epoch 8/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3263 - accuracy: 0.8876 - val_loss: 0.3314 - val_accuracy: 0.8839\n",
            "Epoch 9/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3155 - accuracy: 0.8919 - val_loss: 0.3176 - val_accuracy: 0.8854\n",
            "Epoch 10/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.3059 - accuracy: 0.8940 - val_loss: 0.3126 - val_accuracy: 0.8864\n",
            "Epoch 11/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2979 - accuracy: 0.8968 - val_loss: 0.3067 - val_accuracy: 0.8904\n",
            "Epoch 12/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2912 - accuracy: 0.8995 - val_loss: 0.3054 - val_accuracy: 0.8932\n",
            "Epoch 13/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2857 - accuracy: 0.9021 - val_loss: 0.2956 - val_accuracy: 0.8947\n",
            "Epoch 14/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2800 - accuracy: 0.9022 - val_loss: 0.2895 - val_accuracy: 0.8950\n",
            "Epoch 15/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2753 - accuracy: 0.9038 - val_loss: 0.2852 - val_accuracy: 0.8990\n",
            "Epoch 16/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2706 - accuracy: 0.9068 - val_loss: 0.2825 - val_accuracy: 0.9005\n",
            "Epoch 17/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2665 - accuracy: 0.9077 - val_loss: 0.2788 - val_accuracy: 0.9002\n",
            "Epoch 18/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2621 - accuracy: 0.9085 - val_loss: 0.2772 - val_accuracy: 0.9015\n",
            "Epoch 19/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2587 - accuracy: 0.9111 - val_loss: 0.2754 - val_accuracy: 0.9047\n",
            "Epoch 20/20\n",
            "1376/1376 [==============================] - 3s 2ms/step - loss: 0.2549 - accuracy: 0.9127 - val_loss: 0.2741 - val_accuracy: 0.9045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_A\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: my_model_A\\assets\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model_A = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
        "                      validation_data=(X_valid_A, y_valid_A))\n",
        "model_A.save(\"my_model_A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwFiJGtXJJmQ",
        "outputId": "25ece2fe-5dd7-4a00-fb9b-f2e4bfc11aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 1s 39ms/step - loss: 0.8813 - accuracy: 0.1500 - val_loss: 0.8283 - val_accuracy: 0.1780\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.8425 - accuracy: 0.1650 - val_loss: 0.7982 - val_accuracy: 0.1978\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.8090 - accuracy: 0.2000 - val_loss: 0.7716 - val_accuracy: 0.2305\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7766 - accuracy: 0.2050 - val_loss: 0.7503 - val_accuracy: 0.2928\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.7536 - accuracy: 0.2750 - val_loss: 0.7295 - val_accuracy: 0.3680\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.7315 - accuracy: 0.3850 - val_loss: 0.7110 - val_accuracy: 0.4392\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.7120 - accuracy: 0.4450 - val_loss: 0.6939 - val_accuracy: 0.5163\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6960 - accuracy: 0.5100 - val_loss: 0.6786 - val_accuracy: 0.5806\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6791 - accuracy: 0.6100 - val_loss: 0.6648 - val_accuracy: 0.6380\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6640 - accuracy: 0.6750 - val_loss: 0.6522 - val_accuracy: 0.6716\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6499 - accuracy: 0.7150 - val_loss: 0.6401 - val_accuracy: 0.7112\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6365 - accuracy: 0.7700 - val_loss: 0.6277 - val_accuracy: 0.7488\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.6233 - accuracy: 0.8050 - val_loss: 0.6173 - val_accuracy: 0.7636\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.6114 - accuracy: 0.8300 - val_loss: 0.6068 - val_accuracy: 0.7784\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.5999 - accuracy: 0.8550 - val_loss: 0.5957 - val_accuracy: 0.8111\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.5884 - accuracy: 0.8850 - val_loss: 0.5852 - val_accuracy: 0.8249\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.5771 - accuracy: 0.8950 - val_loss: 0.5756 - val_accuracy: 0.8358\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.5665 - accuracy: 0.9000 - val_loss: 0.5642 - val_accuracy: 0.8744\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.5549 - accuracy: 0.9200 - val_loss: 0.5542 - val_accuracy: 0.8823\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.5450 - accuracy: 0.9350 - val_loss: 0.5451 - val_accuracy: 0.8882\n"
          ]
        }
      ],
      "source": [
        "# training set B(setB has two labels, this is why we use binary_crossentropy)\n",
        "tf.random.set_seed(42)\n",
        "model_B = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_B.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
        "                      validation_data=(X_valid_B, y_valid_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mpC2apfJJmQ",
        "outputId": "1dd4be63-3e34-4978-960f-f04cd5b1ed2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.8825\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.555753231048584, 0.8824999928474426]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_B.evaluate(X_test_B, y_test_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujfPpfbbJJmQ"
      },
      "source": [
        "Now let's try reusing the pretrained model A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuE0dG8cJJmQ"
      },
      "outputs": [],
      "source": [
        "model_A = tf.keras.models.load_model(\"my_model_A\")\n",
        "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoaIYqxJJmR"
      },
      "source": [
        "Note that `model_B_on_A` and `model_A` actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build `model_B_on_A` on top of a *clone* of `model_A`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EshmzHW0JJmR"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model_A_clone = tf.keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "\n",
        "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
        "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okZLji1XJJmR",
        "outputId": "de98121f-99f6-49df-c9e6-a3a36451e8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "7/7 [==============================] - 1s 43ms/step - loss: 2.3954 - accuracy: 0.4450 - val_loss: 1.7080 - val_accuracy: 0.4847\n",
            "Epoch 2/4\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 1.5009 - accuracy: 0.4450 - val_loss: 1.0124 - val_accuracy: 0.4906\n",
            "Epoch 3/4\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.9050 - accuracy: 0.4450 - val_loss: 0.7103 - val_accuracy: 0.5272\n",
            "Epoch 4/4\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.6951 - accuracy: 0.5600 - val_loss: 0.6753 - val_accuracy: 0.5757\n",
            "Epoch 1/16\n",
            "7/7 [==============================] - 1s 36ms/step - loss: 0.6495 - accuracy: 0.6600 - val_loss: 0.6274 - val_accuracy: 0.6261\n",
            "Epoch 2/16\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.6054 - accuracy: 0.7250 - val_loss: 0.5759 - val_accuracy: 0.7577\n",
            "Epoch 3/16\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5564 - accuracy: 0.7950 - val_loss: 0.5517 - val_accuracy: 0.7784\n",
            "Epoch 4/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.5146 - accuracy: 0.8350 - val_loss: 0.5151 - val_accuracy: 0.8348\n",
            "Epoch 5/16\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.4821 - accuracy: 0.8400 - val_loss: 0.4789 - val_accuracy: 0.8783\n",
            "Epoch 6/16\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4533 - accuracy: 0.8850 - val_loss: 0.4532 - val_accuracy: 0.9021\n",
            "Epoch 7/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.4263 - accuracy: 0.9100 - val_loss: 0.4352 - val_accuracy: 0.8902\n",
            "Epoch 8/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.4140 - accuracy: 0.9150 - val_loss: 0.4147 - val_accuracy: 0.9130\n",
            "Epoch 9/16\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.3890 - accuracy: 0.9400 - val_loss: 0.3973 - val_accuracy: 0.9327\n",
            "Epoch 10/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.3718 - accuracy: 0.9450 - val_loss: 0.3837 - val_accuracy: 0.9337\n",
            "Epoch 11/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3570 - accuracy: 0.9350 - val_loss: 0.3680 - val_accuracy: 0.9397\n",
            "Epoch 12/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3426 - accuracy: 0.9450 - val_loss: 0.3547 - val_accuracy: 0.9426\n",
            "Epoch 13/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3278 - accuracy: 0.9500 - val_loss: 0.3464 - val_accuracy: 0.9377\n",
            "Epoch 14/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3185 - accuracy: 0.9450 - val_loss: 0.3360 - val_accuracy: 0.9416\n",
            "Epoch 15/16\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3088 - accuracy: 0.9500 - val_loss: 0.3251 - val_accuracy: 0.9436\n",
            "Epoch 16/16\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.2981 - accuracy: 0.9500 - val_loss: 0.3187 - val_accuracy: 0.9436\n"
          ]
        }
      ],
      "source": [
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
        "                           validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
        "                           validation_data=(X_valid_B, y_valid_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw26uEmOJJmR",
        "outputId": "e36ca9ea-80d5-48c6-8c09-7b23dba2f272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.8825\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.555753231048584, 0.8824999928474426]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_B.evaluate(X_test_B, y_test_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgnkrYjvJJmR",
        "outputId": "f2b6635b-a89f-45d3-f31c-64c9cf45d7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.9370\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.3261830806732178, 0.9369999766349792]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_B_on_A.evaluate(X_test_B, y_test_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__XyJ7XJJJmR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow_cpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}